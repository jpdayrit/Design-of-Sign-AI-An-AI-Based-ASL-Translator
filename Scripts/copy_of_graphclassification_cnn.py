# -*- coding: utf-8 -*-
"""Copy of GraphClassification_CNN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m38O0rxuRoQym4_CpR2-YFP5Q9q9DFMj
"""

import numpy as np
import matplotlib.pyplot as plt
import os
import cv2
from tqdm import tqdm

DATADIR = "/content/drive/My Drive/PseudoDataTrain2"

CATEGORIES = ["type_1", "type_2", "type_3"]

from google.colab import drive
drive.mount('/content/drive')

training_data = []
IMG_WIDTH = 300
IMG_HEIGHT = 300

def create_training_data():
  for category in CATEGORIES:  
    path = os.path.join(DATADIR,category)  
    class_num = CATEGORIES.index(category)  # if 0, type 1 data; 1, type 2 data; 2, type 3 data. 
    for img in os.listdir(path):  
        img_array = cv2.imread(os.path.join(path,img))  # convert to array
        new_array = cv2.resize(img_array, (IMG_WIDTH, IMG_HEIGHT))
        training_data.append([new_array, class_num])

create_training_data()

print(len(training_data))

DATADIR_VAL = "/content/drive/My Drive/PseudoDataValid2"

validation_data = []

def create_validation_data():
  for category in CATEGORIES:  
    path = os.path.join(DATADIR_VAL,category)  
    class_num = CATEGORIES.index(category)  # if 0, type 1 data; 1, type 2 data; 3, type 3 data. 
    for img in os.listdir(path):  
        img_array = cv2.imread(os.path.join(path,img))  # convert to array
        new_array = cv2.resize(img_array, (IMG_WIDTH, IMG_HEIGHT))
        validation_data.append([new_array, class_num])

create_validation_data()

print(len(validation_data))

import random 

random.shuffle(training_data)
random.shuffle(validation_data)

X = []
y = []

X_val = []
y_val = []

for features, label in training_data:
  X.append(features)
  y.append(label)

X = np.array(X).reshape(-1, IMG_WIDTH, IMG_HEIGHT, 3)

for features, label in validation_data:
  X_val.append(features)
  y_val.append(label)

X_val = np.array(X_val).reshape(-1, IMG_WIDTH, IMG_HEIGHT, 3)

import pickle

pickle_out = open('X.pickle', 'wb')
pickle.dump(X, pickle_out)  # dumps X into pickle_out
pickle_out.close()

pickle_out = open('y.pickle', 'wb')
pickle.dump(y, pickle_out)  # dumps y into pickle_out
pickle_out.close()

pickle_out = open('X_val.pickle', 'wb')
pickle.dump(X_val, pickle_out)  # dumps X_val into pickle_out
pickle_out.close()

pickle_out = open('y_val.pickle', 'wb')
pickle.dump(y_val, pickle_out)  # dumps y_val into pickle_out
pickle_out.close()

from tensorflow.keras import layers, models
import pickle

train_images = pickle.load(open('X.pickle', 'rb'))  # the data
train_labels = pickle.load(open('y.pickle', 'rb'))  # labels for each data

test_images = pickle.load(open('X_val.pickle', 'rb'))  # the data
test_labels = pickle.load(open('y_val.pickle', 'rb'))  # labels for each data

train_images, test_images = train_images / 255.0, test_images / 255.0

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))

model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(3, activation='softmax'))

model.summary()

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_images, train_labels, epochs=10, 
                    validation_data=(test_images, test_labels))

DATADIR_EVAL = "/content/drive/My Drive/PseudoDataReal2"

evaluation_data = []

def create_evaluation_data():
  for category in CATEGORIES:  
    path = os.path.join(DATADIR_EVAL,category)  
    class_num = CATEGORIES.index(category)  # if 0, type 1 data; 1, type 2 data; 2, type 3 data. 
    for img in os.listdir(path):  
        img_array = cv2.imread(os.path.join(path,img))  # convert to array
        new_array = cv2.resize(img_array, (IMG_WIDTH, IMG_HEIGHT))
        evaluation_data.append([new_array, class_num])

create_evaluation_data()

print(len(evaluation_data))

X_eval = []
y_eval = []

random.shuffle(evaluation_data)

for features, label in evaluation_data:
  X_eval.append(features)
  y_eval.append(label)

X_eval = np.array(X_eval).reshape(-1, IMG_WIDTH, IMG_HEIGHT, 3)
X_eval = X_eval/255.0

model.evaluate(X_eval,  y_eval, verbose=1)